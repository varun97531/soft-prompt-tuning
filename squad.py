# -*- coding: utf-8 -*-
"""squad.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SGhYhMuBFP1ynieO0axNtkP_QjbHttW8
"""

import os
import torch
import torch.nn as nn
import time
import csv
import pandas as pd
from torch.utils.data import DataLoader
import torch.nn.functional as func
from bs4 import BeautifulSoup
import numpy as np
from nltk.tokenize import word_tokenize
import string
import nltk
nltk.download('punkt')

import re
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
from torch.utils.data import Dataset
import pandas as pd
from torch.optim import Adam
from tqdm.auto import tqdm

from google.colab import drive
drive.mount('/content/gdrive')

!pip install datasets -q

!pip install transformers -q

import pandas as pd
from datasets import load_dataset

# squad_dataset = load_dataset("squad_v2")
squad_dataset = load_dataset("squad")
train_dataset = squad_dataset["train"]
df = pd.DataFrame(train_dataset)

# df[df['title']=='BeyoncÃ©']
df.head()

# path = r"/content/gdrive/MyDrive/nlp dataset/dataset/train-v2.0.json"
# with open(path, 'r') as file:
#     data = json.load(file)

# print(data.keys())

# len(data['data']), len(data['data'][0]['paragraphs'])

# (data['data'][0].keys()), data['data'][0]['paragraphs'][1].keys()

# len(data['data'][0]['paragraphs'][1]['qas'])

# data['data'][0]['title']

# data['data'][0]['paragraphs'][0]['qas'][0]

# data['data'][0]



from transformers import GPT2LMHeadModel, GPT2TokenizerFast
import torch
import torch.nn as nn

class SoftEmbedding(nn.Module):
  def __init__(self, wte, n_tokens):
      super(SoftEmbedding, self).__init__()
      self.wte = wte
      self.n_tokens = n_tokens
      self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte, n_tokens))

  def initialize_embedding(self, wte, n_token):
      return self.wte.weight[:n_tokens].clone().detach()

  def forward(self, tokens):
      input_embedding = self.wte(tokens[:, self.n_tokens:])
      learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)
      return torch.cat([learned_embedding, input_embedding], 1)

tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained('gpt2')

n_tokens=2

s_wte = SoftEmbedding(model.get_input_embeddings(),
                      n_tokens=n_tokens)

print(s_wte)

model.set_input_embeddings(s_wte)

print(model.get_input_embeddings())

inputs = tokenizer("May the force be", return_tensors="pt")

# need to pad attention_mask and input_ids to be full seq_len + n_learned_tokens
# even though it does not matter what you pad input_ids with, it's just to make HF happy
inputs['input_ids'] = torch.cat([torch.full((1,n_tokens), 50256), inputs['input_ids']], 1)
inputs['attention_mask'] = torch.cat([torch.full((1,n_tokens), 1), inputs['attention_mask']], 1)

outputs = model(**inputs)

print(outputs["logits"].shape)





data = {}
maxi_context = 0
for sent in df['context']:
    maxi_context = max(maxi_context, len(sent.split()))
    if(len(sent) in data):
        data[len(sent)] += 1
    else:
        data[len(sent)] = 1

print(maxi_context)

len(df)

# data = {}
# maxi_text = 0

# for i, sent in enumerate(df['answers']):
#     print(len(sent['text']), sent['text'])
#     text_length = len(sent['text'][0].split()) if len(sent['text']) > 0 else 0
#     maxi_text = max(maxi_text, text_length)

#     if text_length in data:
#         data[text_length] += 1
#     else:
#         data[text_length] = 1

# print("Maximum text length:", maxi_text)
# print("Length distribution:", data)


data = {}
maxi_text = 0
for i, sent in enumerate(df['answers']):
    maxi_text = max(maxi_text, len(sent['text'][0].split()))
    if(len(sent) in data):
        data[len(sent)] += 1
    else:
        data[len(sent)] = 1

print(maxi_text)

df.columns

df.head()

df['text'] = df['answers'].apply(lambda x: x['text'][0])

# Drop the original column if needed
# df = df.drop('answers', axis=1)

df.head()

df_new = df[['question', 'text']]

import torch
from torch.utils.data import DataLoader
from transformers import GPT2Model, GPT2Tokenizer, GPT2Config, AdamW
from torch.nn import Embedding
from datasets import load_dataset
from tqdm import tqdm

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from transformers import GPT2LMHeadModel, GPT2TokenizerFast
model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")

tokenizer.add_special_tokens({"pad_token": "<pad>",
                              "bos_token": "<sos>",
                              "eos_token": "<eos>"
                             })
model.config.pad_token_id = tokenizer.pad_token_id
model.config.bos_token_id = tokenizer.bos_token_id
model.config.eos_token_id = tokenizer.eos_token_id

tokenizer.add_tokens(["<extract>"])
model.resize_token_embeddings(len(tokenizer))

n_tokens = 1

s_wte = SoftEmbedding(model.get_input_embeddings(), n_tokens=n_tokens)
model.set_input_embeddings(s_wte)

if torch.cuda.is_available():
    model = model.cuda()

# Fine-tuning parameters
batch_size = 4
epochs = 5
learning_rate = 1e-5
gradient_clip_value = 1.0

len(df_new)

class Squad(Dataset):
  def __init__(self, squad, tokenizer):
    self.data = squad
    self.question = self.data['question']
    self.text = self.data['text']
    self.X = []

    for question, text in zip(self.question, self.text):
        self.X.append("<sos> " + question + " <extract> " + text + " <eos>")

    self.encode = tokenizer(self.X, max_length=100, truncation=True, padding="max_length", return_tensors="pt")
    # self.encode = torch.cat([torch.full((1,n_tokens), 50261), self.encode], 1)
    self.attention_mask = self.encode['attention_mask']
    self.input_ids = self.encode['input_ids']

  def __len__(self):
    return len(self.X)

  def __getitem__(self, idx):
    return (self.input_ids[idx], self.attention_mask[idx])

dataset = Squad(df_new[:10000], tokenizer)
dataloader = DataLoader(dataset, batch_size=1)

batch_size = 1

dataset_val = Squad(df[10000:11000], tokenizer)
dataloader_val = DataLoader(dataset_val, batch_size=1)

for batch in dataloader:
    print(len(batch[0][0]), batch[0][0])
    print(len(batch[1][0]), torch.sum(batch[1][0]), batch[1][0])
    break

optim = Adam(model.parameters(), lr=1e-3)
device = 'cuda'

parameters = list(model.parameters())
for x in parameters[1:]:
    x.requires_grad = False

epochs = 1
total_loss = 0
num_batches = 0
losses = []
rouges = []
optim = Adam(model.parameters(), lr=1e-1)

model = model.to('cuda')
for epoch in range(epochs):
    for ques, ans in tqdm(dataloader):
        # print(ques.shape)
        # print(ans.shape)
        ques = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ques], 1)
        ans = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ans], 1)

        ques = ques.to(device)
        ans = ans.to(device)

        optim.zero_grad()
        outputs = model(ques, attention_mask=ans, labels=ques)
        loss = outputs.loss
        # print(loss)
        loss.backward()
        optim.step()

        total_loss += loss.item()
        num_batches += 1
        losses.append(loss.item() / batch_size)
    avg_loss = total_loss / num_batches
    print(f'Epoch: {epoch+1}, Loss: {avg_loss:.4f}')
    if avg_loss <= 0.155:
        model.save_pretrained('gpt2_squad')


    val_loss = 0
    num_batches = 0
    for ques, ans in tqdm(dataloader_val):
        ques = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ques], 1)
        ans = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ans], 1)

        ques = ques.to(device)
        ans = ans.to(device)

        optim.zero_grad()
        outputs = model(ques, attention_mask=ans, labels=ques)
        loss = outputs.loss
        optim.step()

        val_loss += loss.item()
        num_batches += 1
        # losses.append(loss.item() / batch_size)
    avg_loss = val_loss / num_batches
    print(f'Epoch: {epoch+1}, VAL Loss: {avg_loss:.4f}')

import matplotlib.pyplot as plt

# Example data (replace with your actual data)
# result = [10, 15, 20, 18, 25]

# Plot the data
plt.plot(losses[::100], marker='o')
plt.xlabel('batches')
plt.ylabel('loss')
plt.title('1D List Plot')
plt.grid(True)
plt.show()

"""Different Hyperparameter"""

epochs = 2
total_loss = 0
num_batches = 0
losses = []
rouges = []
optim = Adam(model.parameters(), lr=1e-1)

model = model.to('cuda')
for epoch in range(epochs):
    for ques, ans in tqdm(dataloader):
        # print(ques.shape)
        # print(ans.shape)
        ques = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ques], 1)
        ans = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ans], 1)

        ques = ques.to(device)
        ans = ans.to(device)

        optim.zero_grad()
        outputs = model(ques, attention_mask=ans, labels=ques)
        loss = outputs.loss
        # print(loss)
        loss.backward()
        optim.step()

        total_loss += loss.item()
        num_batches += 1
        losses.append(loss.item() / batch_size)
    avg_loss = total_loss / num_batches
    print(f'Epoch: {epoch+1}, Loss: {avg_loss:.4f}')
    if avg_loss <= 0.155:
        model.save_pretrained('gpt2_squad')


    val_loss = 0
    num_batches = 0
    for ques, ans in tqdm(dataloader_val):
        ques = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ques], 1)
        ans = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ans], 1)

        ques = ques.to(device)
        ans = ans.to(device)

        optim.zero_grad()
        outputs = model(ques, attention_mask=ans, labels=ques)
        loss = outputs.loss
        optim.step()

        val_loss += loss.item()
        num_batches += 1
        # losses.append(loss.item() / batch_size)
    avg_loss = val_loss / num_batches
    print(f'Epoch: {epoch+1}, VAL Loss: {avg_loss:.4f}')

import matplotlib.pyplot as plt

# Example data (replace with your actual data)
# result = [10, 15, 20, 18, 25]

# Plot the data
plt.plot(losses[::100], marker='o')
plt.xlabel('batches')
plt.ylabel('loss')
plt.title('1D List Plot')
plt.grid(True)
plt.show()

trained_parameter = parameters[0]

model.save_pretrained("/content/gdrive/MyDrive/nlp dataset/gpt2_squad.pth")

no_of_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(no_of_parameters)

trained_parameter

def inference(input_text, model):
    model = model.to('cpu')
    input_ids = tokenizer.encode(input_text, return_tensors='pt')
    ques = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), input_ids], 1)
    # print(input_ids)
    output_ids = model.generate(input_ids, max_length=200, num_beams=5, no_repeat_ngram_size=2, top_k=50)
    output_ids = output_ids
    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return output_text

from nltk.translate.bleu_score import sentence_bleu

def find_bleu(reference_tokens, candidate_tokens):
    print("required : ", reference_tokens)
    print("Predicted : ", candidate_tokens)
    mini = min(len(candidate_tokens.split()), len(reference_tokens.split()))
    reference_tokens = [token.lower() for token in reference_tokens.split()]
    candidate_tokens = [token.lower() for token in candidate_tokens.split()]

    # Calculate BLEU score
    bleu_score = sentence_bleu([reference_tokens], candidate_tokens)

    # Print the BLEU score
    print("BLEU Score:", bleu_score)

for i in range(110,134):
    # print("question :", df_new.loc[i]['question'])
    find_bleu(df_new.loc[i]['text'], inference(df_new.loc[i]['question'], model))

