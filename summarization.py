# -*- coding: utf-8 -*-
"""summarization

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Yprne2ZrTPkYKLmW8s_lJBC03fmm-85P
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install datasets
# !pip install transformers

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import re
# from transformers import GPT2LMHeadModel, GPT2TokenizerFast
# from torch.utils.data import DataLoader
# import torch
# import torch.nn as nn
# from datasets import Dataset as ds
# from torch.utils.data import Dataset
# import pandas as pd
# from torch.optim import Adam
# from tqdm.auto import tqdm

from google.colab import drive
drive.mount('/content/gdrive')

device = torch.device("cuda:0")
print(device)

# Import necessary libraries
from tensorflow.python.client import device_lib

# Get the GPU device name
device_lib.list_local_devices()

import csv
cnn_dataset = {'train': {}, 'test': {}, 'validation':{}}

train_csv_file = r"/content/gdrive/MyDrive/nlp dataset/archive/cnn_dailymail/train.csv"
test_csv_file = r"/content/gdrive/MyDrive/nlp dataset/archive/cnn_dailymail/test.csv"
val_csv_file = r"/content/gdrive/MyDrive/nlp dataset/archive/cnn_dailymail/validation.csv"

# Function to load data from a CSV file into the dataset
def load_data(csv_file, split):
  with open(csv_file, mode='r', encoding='utf-8') as file:
    csv_reader = csv.DictReader(file)

    id_list = []
    article_list = []
    highlights_list = []

    for row in csv_reader:
      id = row['id']
      article = row['article']
      highlights = row['highlights']

      id_list.append(id)
      article_list.append(article)
      highlights_list.append(highlights)

    if(split=='train'):
      cnn_dataset[split]['id'] = id_list
      cnn_dataset[split]['article'] = article_list
      cnn_dataset[split]['highlights'] = highlights_list

      cnn_dataset[split]['id'] = cnn_dataset[split]['id'][:2871]
      cnn_dataset[split]['article'] = cnn_dataset[split]['article'][:2871]
      cnn_dataset[split]['highlights'] = cnn_dataset[split]['highlights'][:2871]

    elif(split=='test'):
      cnn_dataset[split]['id'] = id_list
      cnn_dataset[split]['article'] = article_list
      cnn_dataset[split]['highlights'] = highlights_list

      cnn_dataset[split]['id'] = cnn_dataset[split]['id'][:114]
      cnn_dataset[split]['article'] = cnn_dataset[split]['article'][:114]
      cnn_dataset[split]['highlights'] = cnn_dataset[split]['highlights'][:114]

    elif(split=='validation'):
      cnn_dataset[split]['id'] = id_list
      cnn_dataset[split]['article'] = article_list
      cnn_dataset[split]['highlights'] = highlights_list

      cnn_dataset[split]['id'] = cnn_dataset[split]['id'][:133]
      cnn_dataset[split]['article'] = cnn_dataset[split]['article'][:133]
      cnn_dataset[split]['highlights'] = cnn_dataset[split]['highlights'][:133]

# Load train data
load_data(train_csv_file, 'train')

# Load test data
load_data(test_csv_file, 'test')

# Load val data
load_data(val_csv_file, 'validation')

cnn_dataset.keys()

class SoftEmbedding(nn.Module):
  def __init__(self, wte: nn.Embedding, n_tokens: int = 10, random_range: float = 0.5, initialize_from_vocab: bool = True):
      super(SoftEmbedding, self).__init__()
      self.wte = wte
      self.n_tokens = n_tokens
      self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte, n_tokens, random_range, initialize_from_vocab))

  def initialize_embedding(self,
                            wte: nn.Embedding,
                            n_tokens: int = 10,
                            random_range: float = 0.5, initialize_from_vocab: bool = True):
      if initialize_from_vocab:
          return self.wte.weight[:n_tokens].clone().detach()
      return torch.FloatTensor(n_tokens, wte.weight.size(1)).uniform_(-random_range, random_range)

  def forward(self, tokens):
      input_embedding = self.wte(tokens[:, self.n_tokens:])
      learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)
      return torch.cat([learned_embedding, input_embedding], 1)

model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")

tokenizer.add_special_tokens({"pad_token": "<pad>",
                              "bos_token": "<sos>",
                              "eos_token": "<eos>"
                             })
model.config.pad_token_id = tokenizer.pad_token_id
model.config.bos_token_id = tokenizer.bos_token_id
model.config.eos_token_id = tokenizer.eos_token_id

tokenizer.add_tokens(["<extract>"])
model.resize_token_embeddings(len(tokenizer))

n_tokens = 1

s_wte = SoftEmbedding(model.get_input_embeddings(), n_tokens=n_tokens, initialize_from_vocab=True)
model.set_input_embeddings(s_wte)

if torch.cuda.is_available():
    model = model.cuda()

"""Dataset Class"""

val = tokenizer("Grand Forks and Jamestown to the hepatitis A virus", max_length=20, truncation=True, padding="max_length", return_tensors="pt")

print(val.keys())

print(val["input_ids"])
print(val["attention_mask"])

model

class SummaryDataset(Dataset):
  def __init__(self, cnn_dataset, tokenizer):
    self.data = cnn_dataset
    self.article = self.data['article']
    self.highlights = self.data['highlights']

    self.X = []

    for article, highlights in zip(self.article, self.highlights):
        self.X.append("<sos> " + article + " <extract> " + " ".join(highlights.split("\n")).strip() + " <eos>")

    # print(self.X[0])
    # print("="*80)
    # print(self.article[0])
    # print("="*80)
    # print(" ".join(self.highlights[0].split("\n")))
    # print("="*80)

    self.encode = tokenizer(self.X, max_length=1023, truncation=True, padding="max_length", return_tensors="pt")
    # self.encode = torch.cat([torch.full((1,n_tokens), 50261), self.encode], 1)
    self.attention_mask = self.encode['attention_mask']
    self.input_ids = self.encode['input_ids']
    # self.attention_mask = torch.cat([torch.full((1,n_tokens), 1), self.attention_mask], 1)

    # print("len(self.encode)", len(self.encode))
    # print(self.X[0])

  def __len__(self):
    return len(self.X)

  def __getitem__(self, idx):
    return (self.input_ids[idx], self.attention_mask[idx])

dataset = SummaryDataset(cnn_dataset['train'], tokenizer)

batch_size = 1

dataloader = DataLoader(dataset, batch_size=batch_size)

dataset_val = SummaryDataset(cnn_dataset['test'], tokenizer)
dataloader_val = DataLoader(dataset_val, batch_size=batch_size)

for batch in dataloader:
    print(len(batch[0][0]), batch[0][0])
    print(len(batch[1][0]), torch.sum(batch[1][0]), batch[1][0])
    break

"""Training Loop"""

optim = Adam(model.parameters(), lr=1e-3)

device

parameters = list(model.parameters())
for x in parameters[1:]:
    x.requires_grad = False

epochs = 2
total_loss = 0
num_batches = 0
losses = []
rouges = []

for epoch in range(epochs):
    for ques, ans in tqdm(dataloader):
        ques = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ques], 1)
        ans = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ans], 1)

        ques = ques.to(device)
        ans = ans.to(device)

        optim.zero_grad()
        outputs = model(ques, attention_mask=ans, labels=ques)
        loss = outputs.loss
        # print(loss)
        loss.backward()
        optim.step()

        total_loss += loss.item()
        num_batches += 1
        losses.append(loss.item() / batch_size)
    avg_loss = total_loss / num_batches
    print(f'Epoch: {epoch+1}, Loss: {avg_loss:.4f}')
    if avg_loss <= 0.155:
        model.save_pretrained('gpt2_cnn_daily')


    val_loss = 0
    num_batches = 0
    for ques, ans in tqdm(dataloader_val):
        ques = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ques], 1)
        ans = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ans], 1)

        ques = ques.to(device)
        ans = ans.to(device)

        optim.zero_grad()
        outputs = model(ques, attention_mask=ans, labels=ques)
        loss = outputs.loss
        optim.step()

        val_loss += loss.item()
        num_batches += 1
        # losses.append(loss.item() / batch_size)
    avg_loss = val_loss / num_batches
    print(f'Epoch: {epoch+1}, VAL Loss: {avg_loss:.4f}')

import matplotlib.pyplot as plt

# Example data (replace with your actual data)
# result = [10, 15, 20, 18, 25]

# Plot the data
plt.plot(losses[::500], marker='o')
plt.xlabel('batches')
plt.ylabel('loss')
plt.title('1D List Plot')
plt.grid(True)
plt.show()

epochs = 1
total_loss = 0
num_batches = 0
losses = []
rouges = []
optim = Adam(model.parameters(), lr=1e-1)

model = model.to('cuda')
for epoch in range(epochs):
    for ques, ans in tqdm(dataloader):
        ques = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ques], 1)
        ans = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ans], 1)

        ques = ques.to(device)
        ans = ans.to(device)

        optim.zero_grad()
        outputs = model(ques, attention_mask=ans, labels=ques)
        loss = outputs.loss
        # print(loss)
        loss.backward()
        optim.step()

        total_loss += loss.item()
        num_batches += 1
        losses.append(loss.item() / batch_size)
    avg_loss = total_loss / num_batches
    print(f'Epoch: {epoch+1}, Loss: {avg_loss:.4f}')
    if avg_loss <= 0.155:
        model.save_pretrained('gpt2_cnn_daily')


    val_loss = 0
    num_batches = 0
    for ques, ans in tqdm(dataloader_val):
        ques = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ques], 1)
        ans = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ans], 1)

        ques = ques.to(device)
        ans = ans.to(device)

        optim.zero_grad()
        outputs = model(ques, attention_mask=ans, labels=ques)
        loss = outputs.loss
        optim.step()

        val_loss += loss.item()
        num_batches += 1
        # losses.append(loss.item() / batch_size)
    avg_loss = val_loss / num_batches
    print(f'Epoch: {epoch+1}, VAL Loss: {avg_loss:.4f}')

import matplotlib.pyplot as plt

# Example data (replace with your actual data)
# result = [10, 15, 20, 18, 25]

# Plot the data
plt.plot(losses[::100], marker='o')
plt.xlabel('batches')
plt.ylabel('loss')
plt.title('1D List Plot')
plt.grid(True)
plt.show()

trained_parameter = parameters[0]

model

model.save_pretrained("/content/gdrive/MyDrive/nlp dataset/gpt2_abstractive_extractive.pth")

no_of_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(no_of_parameters)

trained_parameter

def inference(input_text, model):
    model = model.to('cpu')
    input_ids = tokenizer.encode(input_text, return_tensors='pt')
    ques = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), input_ids], 1)
    # print(input_ids)
    output_ids = model.generate(input_ids, max_length=200, num_beams=5, no_repeat_ngram_size=2, top_k=50)
    output_ids = output_ids
    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return output_text

text = """The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially h
undreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A vi
rus in late September and early October"""

inference(text, model)

from nltk.translate.bleu_score import sentence_bleu

def find_bleu(reference_tokens, candidate_tokens):
    print("required : ", reference_tokens)
    print("Predicted : ", candidate_tokens)
    mini = min(len(candidate_tokens.split()), len(reference_tokens.split()))
    reference_tokens = [token.lower() for token in reference_tokens.split()][:mini]
    candidate_tokens = [token.lower() for token in candidate_tokens.split()][:mini]

    # Calculate BLEU score
    bleu_score = sentence_bleu([reference_tokens], candidate_tokens)

    # Print the BLEU score
    print("BLEU Score:", bleu_score)

find_bleu(cnn_dataset['test']['article'][0], inference(cnn_dataset['test']['highlights'][0], model))

find_bleu(cnn_dataset['test']['article'][1], inference(cnn_dataset['test']['highlights'][1], model))

find_bleu(cnn_dataset['test']['article'][2], inference(cnn_dataset['test']['highlights'][2], model))

for i in range(40):
    find_bleu(cnn_dataset['test']['article'][i], inference(cnn_dataset['test']['highlights'][i], model))
    print("="*20)

