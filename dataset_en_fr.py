# -*- coding: utf-8 -*-
"""dataset_en_fr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nNHKZd1W_LIOUYGi_mDg2mIWdl6Y3VxY
"""

import os
import torch
import torch.nn as nn
import time
import csv
import pandas as pd
from torch.utils.data import DataLoader
import torch.nn.functional as func
from bs4 import BeautifulSoup
import numpy as np
from nltk.tokenize import word_tokenize
import string
import nltk
nltk.download('punkt')

import re
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
from torch.utils.data import Dataset
import pandas as pd
from torch.optim import Adam
from tqdm.auto import tqdm

from google.colab import drive
drive.mount('/content/gdrive')

path = r"/content/gdrive/MyDrive/nlp dataset/de-en/europarl-v7.de-en.en"
data_english = r"/content/gdrive/MyDrive/nlp dataset/de-en/europarl-v7.de-en.en"
data_german = r"/content/gdrive/MyDrive/nlp dataset/de-en/europarl-v7.de-en.de"

with open(os.path.join(data_english), "r") as file:
    data_english = file.readlines()

with open(os.path.join(data_german), "r") as file:
    data_german = file.readlines()

punct = string.punctuation
punct.replace('.', '')

def get_lower(text):
    return text.lower()

def remove_punctuations(text):
    return ''.join([char for char in text if char not in punct])

def tokenize(text):
    # text = text.strip()
    return word_tokenize(text)

def remove_alpha_numeric(sentence):
    # return ' '.join(word for word in tokens if word.isalpha())
    words = sentence.split()
    alphabetic_words = [word for word in words if word.isalpha()]
    return ' '.join(alphabetic_words)

def remove_tags(text):
    return BeautifulSoup(text, 'html.parser').get_text()

def remove_extra_gaps(text):
    return ' '.join(text.split())

def pipeline(text):
    text = get_lower(text)
    text = remove_punctuations(text)
    tokens = tokenize(text)
    text = remove_alpha_numeric(text)
    text = remove_tags(text)
    text = remove_extra_gaps(text)
    return text

data = {}
maxi_english = 0
for sent in data_english:
    maxi_english = max(maxi_english, len(sent))
    if(len(sent) in data):
        data[len(sent)] += 1
    else:
        data[len(sent)] = 1

print(maxi_english)

import matplotlib.pyplot as plt


keys = data.keys()
values = data.values()

plt.bar(keys, values)

# Add labels and a title
plt.xlabel('length')
plt.ylabel('count')
plt.title('Histogram of Dictionary Values')

# Show the plot
plt.show()

data = {}
maxi_german = 0
for sent in data_german:
    maxi_german = max(maxi_german, len(sent))
    if(len(sent) in data):
        data[len(sent)] += 1
    else:
        data[len(sent)] = 1

print(maxi_german)

import matplotlib.pyplot as plt


keys = data.keys()
values = data.values()

plt.bar(keys, values)

# Add labels and a title
plt.xlabel('length')
plt.ylabel('count')
plt.title('Histogram of Dictionary Values')

# Show the plot
plt.show()

# df = pd.read_csv("Auguste_Maquet.txt", sep='', header=None, names="sentences")
df = pd.DataFrame({"english": data_english, "german": data_german})
print("Size : ", len(df))
df.head()

rows_to_drop = []
df['english'] = df['english'].str.lower()
df['german'] = df['german'].str.lower()
limit = 300  # For example, set your desired limit

for index, row in df.iterrows():
    if len(row['english']) > limit or len(row['german']) > limit:
        rows_to_drop.append(index)

df = df.drop(rows_to_drop)

print(df)

m1, m2 = 0, 0
for index, row in df.iterrows():
    m1 = max(m1, len(row['english']))
    m2 = max(m2, len(row['german']))
print(m1, m2)

!pip install datasets -q

!pip install transformers -q

class SoftEmbedding(nn.Module):
  def __init__(self, wte, n_tokens):
      super(SoftEmbedding, self).__init__()
      self.wte = wte
      self.n_tokens = n_tokens
      self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte, n_tokens))

  def initialize_embedding(self, wte, n_token):
      return self.wte.weight[:n_tokens].clone().detach()

  def forward(self, tokens):
      input_embedding = self.wte(tokens[:, self.n_tokens:])
      learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)
      return torch.cat([learned_embedding, input_embedding], 1)

from transformers import GPT2LMHeadModel, GPT2TokenizerFast
model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")

tokenizer.add_special_tokens({"pad_token": "<pad>",
                              "bos_token": "<sos>",
                              "eos_token": "<eos>"
                             })
model.config.pad_token_id = tokenizer.pad_token_id
model.config.bos_token_id = tokenizer.bos_token_id
model.config.eos_token_id = tokenizer.eos_token_id

tokenizer.add_tokens(["<extract>"])
model.resize_token_embeddings(len(tokenizer))

n_tokens = 1

s_wte = SoftEmbedding(model.get_input_embeddings(), n_tokens=n_tokens)
model.set_input_embeddings(s_wte)

if torch.cuda.is_available():
    model = model.cuda()

# Fine-tuning parameters
batch_size = 4
epochs = 5
learning_rate = 1e-5
gradient_clip_value = 1.0

model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")

tokenizer.add_special_tokens({"pad_token": "<pad>",
                              "bos_token": "<sos>",
                              "eos_token": "<eos>"
                             })
model.config.pad_token_id = tokenizer.pad_token_id
model.config.bos_token_id = tokenizer.bos_token_id
model.config.eos_token_id = tokenizer.eos_token_id

tokenizer.add_tokens(["<extract>"])
model.resize_token_embeddings(len(tokenizer))

n_tokens = 1

s_wte = SoftEmbedding(model.get_input_embeddings(), n_tokens=n_tokens)
model.set_input_embeddings(s_wte)

if torch.cuda.is_available():
    model = model.cuda()

val = tokenizer("Grand Forks and Jamestown to the hepatitis A virus", max_length=20, truncation=True, padding="max_length", return_tensors="pt")

print(val.keys())

print(val["input_ids"])
print(val["attention_mask"])

model

class english_to_german(Dataset):
  def __init__(self, en2gr, tokenizer):
    self.data = en2gr
    self.english = self.data['english']
    self.german = self.data['german']
    self.X = []

    for english, german in zip(self.english, self.german):
        self.X.append("<sos> " + english + " <extract> " + german + " <eos>")

    self.encode = tokenizer(self.X, max_length=603, truncation=True, padding="max_length", return_tensors="pt")
    # self.encode = torch.cat([torch.full((1,n_tokens), 50261), self.encode], 1)
    self.attention_mask = self.encode['attention_mask']
    self.input_ids = self.encode['input_ids']

  def __len__(self):
    return len(self.X)

  def __getitem__(self, idx):
    return (self.input_ids[idx], self.attention_mask[idx])

dataset = english_to_german(df[:2000], tokenizer)
dataloader = DataLoader(dataset, batch_size=batch_size)

batch_size = 1

dataset_val = english_to_german(df[5000:5200], tokenizer)
dataloader_val = DataLoader(dataset_val, batch_size=batch_size)

for batch in dataloader:
    print(len(batch[0][0]), batch[0][0])
    print(len(batch[1][0]), torch.sum(batch[1][0]), batch[1][0])
    break

optim = Adam(model.parameters(), lr=1e-3)
device = 'cuda'

parameters = list(model.parameters())
for x in parameters[1:]:
    x.requires_grad = False

epochs = 2
total_loss = 0
num_batches = 0
losses = []
rouges = []

for epoch in range(epochs):
    for ques, ans in tqdm(dataloader):
        # print(ques, ques.shape)
        ques = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ques], 1)
        # print(ans, ans.shape)
        ans = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ans], 1)

        ques = ques.to(device)
        ans = ans.to(device)

        optim.zero_grad()
        outputs = model(ques, attention_mask=ans, labels=ques)
        loss = outputs.loss
        # print(loss)
        loss.backward()
        optim.step()

        total_loss += loss.item()
        num_batches += 1
        losses.append(loss.item() / batch_size)
    avg_loss = total_loss / num_batches
    print(f'Epoch: {epoch+1}, Loss: {avg_loss:.4f}')
    if avg_loss <= 0.155:
        model.save_pretrained('gpt2_en2gr')


    val_loss = 0
    num_batches = 0
    for ques, ans in tqdm(dataloader_val):
        ques = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ques], 1)
        ans = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ans], 1)

        ques = ques.to(device)
        ans = ans.to(device)

        optim.zero_grad()
        outputs = model(ques, attention_mask=ans, labels=ques)
        loss = outputs.loss
        optim.step()

        val_loss += loss.item()
        num_batches += 1
        # losses.append(loss.item() / batch_size)
    avg_loss = val_loss / num_batches
    print(f'Epoch: {epoch+1}, VAL Loss: {avg_loss:.4f}')

import matplotlib.pyplot as plt

# Example data (replace with your actual data)
# result = [10, 15, 20, 18, 25]

# Plot the data
plt.plot(losses[::100], marker='o')
plt.xlabel('batches')
plt.ylabel('loss')
plt.title('1D List Plot')
plt.grid(True)
plt.show()

"""DIFFERENT HYPERPARAMETER"""

epochs = 1
total_loss = 0
num_batches = 0
losses = []
rouges = []
optim = Adam(model.parameters(), lr=1e-1)

model = model.to('cuda')
for epoch in range(epochs):
    for ques, ans in tqdm(dataloader):
        ques = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ques], 1)
        ans = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ans], 1)

        ques = ques.to(device)
        ans = ans.to(device)

        optim.zero_grad()
        outputs = model(ques, attention_mask=ans, labels=ques)
        loss = outputs.loss
        # print(loss)
        loss.backward()
        optim.step()

        total_loss += loss.item()
        num_batches += 1
        losses.append(loss.item() / batch_size)
    avg_loss = total_loss / num_batches
    print(f'Epoch: {epoch+1}, Loss: {avg_loss:.4f}')
    if avg_loss <= 0.155:
        model.save_pretrained('gpt2_cnn_daily')


    val_loss = 0
    num_batches = 0
    for ques, ans in tqdm(dataloader_val):
        ques = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ques], 1)
        ans = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), ans], 1)

        ques = ques.to(device)
        ans = ans.to(device)

        optim.zero_grad()
        outputs = model(ques, attention_mask=ans, labels=ques)
        loss = outputs.loss
        optim.step()

        val_loss += loss.item()
        num_batches += 1
        # losses.append(loss.item() / batch_size)
    avg_loss = val_loss / num_batches
    print(f'Epoch: {epoch+1}, VAL Loss: {avg_loss:.4f}')

import matplotlib.pyplot as plt

# Example data (replace with your actual data)
# result = [10, 15, 20, 18, 25]

# Plot the data
plt.plot(losses[::100], marker='o')
plt.xlabel('batches')
plt.ylabel('loss')
plt.title('1D List Plot')
plt.grid(True)
plt.show()

trained_parameter = parameters[0]

model.save_pretrained("/content/gdrive/MyDrive/nlp dataset/gpt2_en2gr.pth")

no_of_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(no_of_parameters)

trained_parameter

def inference(input_text, model):
    model = model.to('cpu')
    input_ids = tokenizer.encode(input_text, return_tensors='pt')
    ques = torch.cat([torch.full((1, n_tokens), 0, dtype=torch.long), input_ids], 1)
    # print(input_ids)
    output_ids = model.generate(input_ids, max_length=200, num_beams=5, no_repeat_ngram_size=2, top_k=50)
    output_ids = output_ids
    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return output_text

from nltk.translate.bleu_score import sentence_bleu

def find_bleu(reference_tokens, candidate_tokens):
    print("required : ", reference_tokens)
    print("Predicted : ", candidate_tokens)
    mini = min(len(candidate_tokens.split()), len(reference_tokens.split()))
    reference_tokens = [token.lower() for token in reference_tokens.split()]
    candidate_tokens = [token.lower() for token in candidate_tokens.split()]

    # Calculate BLEU score
    bleu_score = sentence_bleu([reference_tokens], candidate_tokens)

    # Print the BLEU score
    print("BLEU Score:", bleu_score)

for i in range(1200,1205):
    find_bleu(df.loc[i]['english'], inference(df.loc[i]['german'], model))

